% the template is adapted from https://github.com/kourgeorge/arxiv-style

\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content

\usepackage{tikz} 
\usetikzlibrary{matrix}

\usepackage{graphicx}
%\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
\usepackage{subcaption}
%\usepackage{booktabs}


\title{Some not that novel approach to image processing using nonparametric statistic}


%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
  Zhuo Yueyi \\ %\thanks{Use footnote for providing further
    %information about author (webpage, alternative
    %address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science and Engineering\\
  Nanjing University of Science and Technology\\
  \texttt{yiyuezhuo@gmail.com} \\
}

\begin{document}
\maketitle

\begin{abstract}
%\lipsum[1]
This is a class paper for nonparametric statistical. 
All of methods introduced by \cite{wasserman2006all} will get a example to show how it can be applied on
image processing problem.
\end{abstract}


% keywords can be removed
\keywords{Nonparametric statistic \and image processing }


\section{Introduction}
%\lipsum[2]
%\lipsum[3]




\subsection{Data format of image}
%\label{sec:headings}
%\lipsum[4] See Section \ref{sec:headings}.

Colored(RGB) image can seen as a 3 variables function  $f(i,j,c)$, where $i,j$ denote the location of image, $c$ denotes
channel(reg, green, blue). The function can be encoded by three matrixes as shown in Fig~\ref{fig:toad_rgb}.

\begin{figure}[htb]
  \centering
  \input{tikzpictures/toad_rgb}
  \caption{Representation: Image as three matrix}
  \label{fig:toad_rgb}
\end{figure}

Since the colored image introduce extra complexity, in this paper the gray image is used for simplicity.
Instead of three matrices, the gray image is represented by a single matrix which is given by:

\begin{equation}
  f_{gray}(i,j) = 0.21 f_{rgb}(i,j,1) + 0.72 f_{rgb}(i,j,2) + 0.07 f_{rgb}(i,j,3)
  \label{eq:rgb_to_gray}
\end{equation}

Sometimes to "flatten" a matrix to a long vector is a useful viewpoint. There're two order to flatten a matrix,
row-major and column-major. The row-major is used in Python(Numpy) and the column-major is used by R and MatLab.
As my preferred language, row-major order of Python is illustrated as Fig~\ref{fig:flatten_matrix}.

\begin{figure}[htb]
  \centering
  \input{tikzpictures/flatten_matrix}
  \caption{Flatten a matrix by row}
  \label{fig:flatten_matrix}
\end{figure}


As a application we can model the vector as i.i.d sample of some distribution using simpler symbol, the most 
common practice is to represent them as a histogram, see Fig~\ref{fig:map_gray_to_hist}.

\begin{figure}[htb]
  \centering
  \input{tikzpictures/map_gray_to_hist}
  \caption{Mapping from gray image to histogram}
  \label{fig:map_gray_to_hist}
\end{figure}

\subsection{Inverse problem in image processing}

As presented in \cite{dong2015image}, we usually regard image as a result of combination of some "observe"
function and noise. In its simplest form, we can write:

\begin{equation}
\mathbf{f} = \mathbf{A} \mathbf{x} + \mathbf{\eta}
\label{eq:inverse_problem}
\end{equation}

where $\mathbf{f}$ denote a (gray) image, $\mathbf{A}$ is a linear operation, $\mathbf{x}$ is "true" image,
$\mathbf{\eta}$ is noise.
 
\section{Smoothing: nonparametric statistic and image processing}

There exists a fully equivalence which can be shown on smoothing problem.

\cite{wasserman2006all} state regression for pairs $(x_1,Y_1,\dots,x_n,Y_n)$ like:

$$
Y_i = r(x_i) + \epsilon_i
$$

Replacing $x_i$ with $x_{ij}=(i,j)$ ($i,j$ denote coordinates of corresponding pixel on image), compared to Eq~\ref{eq:inverse_problem},
so the $r(x_{i,j})=r(i,j)$ denote the complex true image function. $\epsilon_i \to \epsilon_{i,j} \sim N(0,\sigma)$ is i.i.d white noise.
It's obvious that we can not specify a simple parametric model on it (For some possible parametric image model,
see Fig~\ref{fig:parametric_model} ) so that the the noise derived from underestimated model will cause misunderstanding. 
Thus we must employ a nonparametric fashion though computer scientist may not aware it explicitly.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/parametric_model_1.png}
    \caption{$y_{ij} = i+j$}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/parametric_model_2.png}
    \caption{$y_{ij} = \log(1+i)+\log(1+j)$}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/parametric_model_3.png}
    \caption{$y_{ij} = \sin(i)+\sin(j)$}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/parametric_model_4.png}
    \caption{$y_{ij} = \sin(0.1i)+\sin(0.1j)$}
  \end{subfigure}
  \caption{Simple parametric model can't capture big picture of image.}
  \label{fig:parametric_model}
\end{figure}


\subsection{Kernel regression}
%\lipsum[5]
%\begin{equation}
%\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
%\end{equation}

As stated in \cite{solomon2011fundamentals}, a mean filtering, as simplest filter technology, give same weight to neibor pixel to smooth the
noised to remove noise(try to find true$r(i,j)$). Concretely, here's a instance of $3 \times 3$ filtering:

\begin{equation}
g(i,j) = \frac{1}{9} \sum_{di=-1}^1 \sum_{dj=-1}^1 f(i+di,j+dj)
\label{eq:mean_filter}
\end{equation}

If we set kernel function as:

$$ 
K(x_{i,j},y_{i,j}) =  
\begin{cases}
  1, & \text{if $ |i_x - i_y| \le 1$ and $|j_x - j_y| \le 1$} \\
  0, & \text{otherwise}
\end{cases}
$$

Which is a nature two-variables extension from box-car kernel introduced by \cite{wasserman2006all}.
Then the kernel regression given by:

$$
\hat{r}(x_{i,j}) = \frac{\sum_{s} \sum_{t} K(x_{i,j},x_{s,t}) Y_{s,t}}{\sum_{s} \sum_{t} K(x_{i,j},x_{s,t})}
$$

is equivalence to Eq~\ref{eq:mean_filter}. A example can be shown in Fig~\ref{fig:effect_noise_removal}. 
The kernel size(here it's $3$) is directly correspond to bandwidth $h$ used in kernel regression. 
We can see that $h=5$(mean kernel = $11 \times 11 $) lead to over-smooth $\hat{r}$, $h=1$ seems under-smoothed,
the result of median seems fine.

Mean filtering is usually weaker than median filtering, a example is shown in Fig~\ref{fig:effect_noise_removal}. 
Though median filtering is also a linear smoother: 

$$
\hat{r}(x_{i,j})=\sum_s \sum_t l_{s,t}(x_{i,j})Y_{s,t}$$

its hat-matrix can be represented as: 

$$ 
l_{s,t}(x_{i,j}) = I(s=i_{median}) \times I(t=j_{median}) 
$$

Where $i_{median},j_{median}$ denote the coordinates of pixel which is median of neiborhood of given pixel $(i,j)$.

But obviously it can not be included by kernel regression, say, 
we can't find a general kernel function $K(x_{i,j},y_{i,j})$ which give above hat matrix. 

Following textbook \cite{wasserman2006all}, we can try to find different improvment other than mean-filter 
(i.e. gaussian kernel regression(filtering) using gaussian kernel) or rank-filter, 
for example, local polynomial regression. 

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/noise_removal_1.png}
    \caption{Noised toad}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/noise_removal_2.png}
    \caption{Kernel regression(h=1)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/noise_removal_3.png}
    \caption{Kernel regression(h=5)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/noise_removal_4.png}
    \caption{Median regression(h=3)}
  \end{subfigure}
  \caption{$\hat{r}(x_{i,j})$}
  \label{fig:effect_noise_removal}
\end{figure}


\subsection{Local Polynomial Regression}

Basically, polynomial regression fit a "delta" model $f_{i,j}(di,dj)$ for every points $(i,j)$, and use $f(0,0)$
as estimation of $\hat{r}(x_{i,j})$. Especially, the linear formulation with kernel weighting is most 
interesting considering higher order may causing numerical problem.

The vanilla equation,

$$
\mathbf{l}(x) = e_1^T (X_x^T W_x X_x)^{-1} X_x^T W_x
$$

may lead too large matrix $W_x$ ($23520 \times 23520$), so the broadcast is used to represent the matrix as 
$23520$ vector. However, fit a model $23520$ times using slightly different parameters is inevitable. 
The result seems nice more than results given by kernel regression and median filter, 
as shown in Fig~\ref{fig:local_poly} \footnote{The above related code is written in Python and following related code is written in Julia}.  
The "true" mean the model is applied on true image instead of noised image,
we can see the detail is kept finely compared to other methods which may over-smooth even in "true" image.
The right one show $h=1$ result of smoothing, we can see the result may not be better than median regression especially considering
it costly computation time. 

And it can also provide variance information. The second sub-fig show the value of linear, it show the linear
"trend" in that pixel. Taking absolute on it, we can compare it to so called edge detection filter such as Sobel operation and LoG
in image processing, see Fig~\ref{fig:edge}. It's reasonable since the edge is so called high frequency feature, hence the extremely
change matched by different methods will give consistent response as seen in above figure.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/poly_nr.png}
    \caption{Local Poly Reg Cons(true)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/poly_nr_lin.png}
    \caption{Local Poly Reg Lin(true)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/poly_nr_true.png}
    \caption{Local Poly Reg Cons(h=1)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/poly_nr_lin_true.png}
    \caption{Local Poly Reg Lin(h=1)}
  \end{subfigure}
  \caption{Local polynomial regression results}
  \label{fig:local_poly}
\end{figure}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/edge_1.png}
    \caption{LPR linear term coef}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/edge_2.png}
    \caption{LPR linear term abs coef}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/edge_3.png}
    \caption{LoG($\sigma=1$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/edge_4.png}
    \caption{Sobel}
  \end{subfigure}
  \caption{How linear term in LPR can be explained as edge detection}
  \label{fig:edge}
\end{figure}



\subsection{Cross validation for smoothing}

%Since the image usually include too many data point. Using local property appearing in image processing 
%will be helpful when evaluate the leave-one-out cross-validation score.

Since here we have actually the true function $r(i,j)$, the vanilla risk:

$$
R(h) = E(\frac{1}{n}\sum_{i=1}^n(\hat{r}_n(x_i)-r(x_i))^2)
$$

for given $h,\sigma$ (the randomness of $\hat{r}_n(x_i)$ is subject to std paramater of noise term $\sigma$) 
can be approximated by Monte Calor simulation:

$$
R(h) \approx \frac{1}{T} \sum_{t=1}^T \frac{1}{n}\sum_{i=1}^n(\hat{r}_n(x_i)-r(x_i))^2
$$

The $T$ is number of simulation, here $T=200$ is used. See Fig~\ref{fig:crossvalidation}
\footnote{Note Julia treat gray image as $[0,1]$ float number instead of $[0,255]$ in Python. Following content is based on Julia,
so a $sigma=25$ noise in range $[0,255]$ for MatLab/Python is equivalence to $0.098$ in range $[0,1]$ for Julia.}
 for result of simulation with mean with 3 $\sigma$. 
It's obvious that the optimal may located in $h \approx 0.75$ .

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{images/crossvalidation1.png}
    \caption{Monte Carlo $R(h)$ for the true image}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{images/crossvalidation2.png}
    \caption{CV for a given noised image}
  \end{subfigure}
  \caption{Cross Validation}
  \label{fig:crossvalidation}
\end{figure}

Of course usually we don't have true function $r(x_i)$, so we want a sample-based method, 
that is the leave-one-out cross validation score which can be employed to show a guideline to choose optimal bandwidth $h$
when true function is not observed.

Fortunately, the LOOCV is equivalence to remove the center of the kernel used in filtering, so that we can simplify this to 
convolution which can be effectively computed by software.

The vanilla equation:

$$
CV = \hat{R}(h)=\frac{1}{n}\sum_{i=1}^n (Y_i-\hat{r}_{-i}(x_i))^2
$$

is equivalence to following change of kernel of convolution in gaussian kernel regression, see Fig~\ref{fig:gaussian_kernel}. 
The result can seen in Fig~\ref{fig:crossvalidation}(b).

The Equation(5.36) in \cite{wasserman2006all} define the generalized cross validation:

$$
GCV = \frac{1}{n}\sum_{i=1}^n \left(\frac{Y_i-\hat{r}_n(x_i)}{1-\nu/n}\right)^2
$$

Where $\nu = \sum_{i=1}^n \mathbf{L}_{ii}$. Since the kernel used in gaussian kernel regression is homogeneous(i.e. it always take $0.1621$
shown in \ref{fig:gaussian_kernel} when $h=1$.). Therefore $L_{ii} = \nu/n$, thus the GCV take same value as CV take. 

\begin{figure}[htb]
  \centering
  \input{tikzpictures/gaussian_kernel}
  \caption{Gaussian kernel used in Leave-one-out cross validation}
  \label{fig:gaussian_kernel}
\end{figure}


\subsection{Variance estimation introduced by smoothing}

In fact, the image is polluted by some i.i.d noise terms $\epsilon_i \sim N(0,\sigma^2)$ where hold a constant $\sigma$ for every index $i$.
so that, following \cite{wasserman2006all} Theorem 5.85, we can estimate it by following equation:

$$
\hat{\sigma}^2=\frac{\sum_{i=1}^n (Y_i-\hat{r}(x+i))^2}{n-2\nu+\tilde{\nu}}
$$

where

$$
\nu = \mathrm{tr}(\mathbf{L}), \quad \tilde{\nu}=\mathrm{tr}(\mathbf{L}^T\mathbf{L})=\sum_{i=1}^n||\mathcal{l}(x_i)||^2
$$

In our setting, let $K$ denote the kernel matrix
\footnote{The $K$ can be encoded as a sparse 23520 dimension vector $l_i$ which have 25 non-zero value while all other value are zero to match
the notation used in the textbook, but those tedious transform are omitted for briefly.}
such as the one shown in Fig~\ref{fig:gaussian_kernel} leftmost.
$n = 168 \times 140 = 23520$,$\nu = n \times sK[0,0]$,$\tilde{\nu} = \sum_{i=-2}^2\sum_{j=-2}^2 K[i,j]^2$. Set $\sigma=50$(In Julia it's 
$\sigma=0.098$), the simulation distribution of $\hat{\sigma}^2$ can be seen in Fig~\ref{fig:std_est}. 
The interesting thing is that all of them fail to cover true point $0.098$, when $h$ become, the bias is even worsen. It's not that 
surprise considering the consistency condition of estimator, $\nu=o(n),\tilde{\nu}=o(n)$ is not satisfied in this case.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/std_est_1.png}
    \caption{Dist of $\hat{\sigma}^2$ ($h=0.2$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/std_est_2.png}
    \caption{Dist of $\hat{\sigma}^2$ ($h=0.5$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/std_est_3.png}
    \caption{Dist of $\hat{\sigma}^2$ ($h=1.0$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/std_est_4.png}
    \caption{Dist of $\hat{\sigma}^2$ ($h=2.0$)}
  \end{subfigure}
  \caption{simulation distribution of $\hat{\sigma}^2$(samples = 200)}
  \label{fig:std_est}
\end{figure}

\section{Density estimation and threshold segmentation}

Due to the property of linear combination of Eq~\ref{eq:rgb_to_gray}. Splitting the image from RGB to three channel histogram is more useful
compared to using gray histogram directly. For example, the result of toad is shown in Fig~\ref{fig:hist_g_rgb}.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/hist_gray.png}
    \caption{Hist of gray}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/hist_r.png}
    \caption{Hist of red}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/hist_g.png}
    \caption{Hist of green}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/hist_b.png}
    \caption{Hist of blue}
  \end{subfigure}
  \caption{Splitting three channel histogram}
  \label{fig:hist_g_rgb}
\end{figure}

Histogram is subject to bins, we want a smoothed histogram that can either corporate "bandwidth" information and keep 
its continues property(resolution). A choice is Kernel Density Estimation(KDE), Eq(6.26) in \cite{wasserman2006all} show:

$$
\hat{f} = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} K\left( \frac{x-X_i}{h} \right)
$$

I choose Gaussian Kernel, $K(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ and $h=0.3$, the result is shown in Fig~\ref{fig:kde}.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_r.png}
    \caption{KDE of reg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_g.png}
    \caption{KDE of green}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_b.png}
    \caption{KDE of blue}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_code.png}
    \caption{Encoding of KDE}
  \end{subfigure}
  \caption{KDE and threshold encoding}
  \label{fig:kde}
\end{figure}

Some vertical lines are drawn to indicate some "components" behind in the image. We can see some region of the image is segmented firstly
(latent variables) and draw centering color and correlated noise(i.e. MRF) on it. To infer the latent class label in image region,
we can use clustering such as k-mean with a distance measure such as distance of pixel and color. But here, one observe the KDE of image,
manually or automatically choose some cutoff points(Here points is chosen manually, but taking minimum value of KDE numerically 
can be employed.). So in principle we can get $4\times 3\times 3=36$ components, but since RGB is correlated in intensity(say, more red
value imply more green and blue value as well.), the number of valid components is only $18$ as shown in \ref{fig:kde}.
Those components can be splitted into a image, as shown in \ref{fig:splitting}. The processing result of it is called segmentation,
a classic field in image processing. 
Anyway the method using KDE is not that useful compared to modern method using CNN \cite{long2015fully} or 
even early method using MRF\cite{panjwani1995markov}. 

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/toad_split.png}
    \caption{Weird toad splitting}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/toad_split_air.png}
    \caption{Splitting of air}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/toad_split_center.png}
    \caption{Splitting of back}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/toad_split_water.png}
    \caption{Splitting of part of water}
  \end{subfigure}
  \caption{KDE and segmentation}
  \label{fig:splitting}
\end{figure}

\section{Converting Density Estimation into Regression to suppress minimum value made by noise}

Automatically detecting cutoff points through searching on local minimum is subject to some factors. If there is a minimum point, 
but with very tiny curvature, is it a true minimum point or made by random noise? It may occur more usually when $h$ is not "enough".
For example, Fig~\ref{fig:DS_REG} show the KDE of red and blue channels when $h=0.2$ compared to previous $h=0.3$.
We can impose a confidence interval to it to determine. It require technique in \cite{wasserman2006all} section 6.6.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/fake_min_r.png}
    \caption{KDE of red ($h=0.2$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/fake_min_b.png}
    \caption{KDE of blue ($h=0.2$)}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_reg_r.png}
    \caption{Red DS by KR($h=1.0$) with 3$\sigma$ range}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/kde_reg_b.png}
    \caption{Blue DS by KR($h=1.0$) with 3$\sigma$ range}
  \end{subfigure}
  \caption{Some possible fake minimum points and suppressing}
  \label{fig:DS_REG}
\end{figure}


\section{Bootstrapping and confidence of feature detecting}

\section{Conclusion}




\bibliographystyle{unsrt}  
\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``the bibliography'' section.


\end{document}
